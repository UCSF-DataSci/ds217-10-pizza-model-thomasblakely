{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d4dbdea",
   "metadata": {},
   "source": [
    "# Assignment 10: Modeling Fundamentals\n",
    "\n",
    "Complete the following three questions to demonstrate your understanding of statistical modeling, machine learning, and gradient boosting.\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "aeddde37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import xgboost as xgb\n",
    "import os\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(\"output\", exist_ok=True)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6047b91",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fa2f4a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 20640 housing records\n",
      "\n",
      "Feature names: ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude']\n",
      "\n",
      "First few rows:\n",
      "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
      "0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
      "1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
      "2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
      "3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
      "4  3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
      "\n",
      "   Longitude  house_value  \n",
      "0    -122.23        4.526  \n",
      "1    -122.22        3.585  \n",
      "2    -122.24        3.521  \n",
      "3    -122.25        3.413  \n",
      "4    -122.25        3.422  \n",
      "\n",
      "Summary statistics:\n",
      "             MedInc      HouseAge      AveRooms     AveBedrms    Population  \\\n",
      "count  20640.000000  20640.000000  20640.000000  20640.000000  20640.000000   \n",
      "mean       3.870671     28.639486      5.429000      1.096675   1425.476744   \n",
      "std        1.899822     12.585558      2.474173      0.473911   1132.462122   \n",
      "min        0.499900      1.000000      0.846154      0.333333      3.000000   \n",
      "25%        2.563400     18.000000      4.440716      1.006079    787.000000   \n",
      "50%        3.534800     29.000000      5.229129      1.048780   1166.000000   \n",
      "75%        4.743250     37.000000      6.052381      1.099526   1725.000000   \n",
      "max       15.000100     52.000000    141.909091     34.066667  35682.000000   \n",
      "\n",
      "           AveOccup      Latitude     Longitude   house_value  \n",
      "count  20640.000000  20640.000000  20640.000000  20640.000000  \n",
      "mean       3.070655     35.631861   -119.569704      2.068558  \n",
      "std       10.386050      2.135952      2.003532      1.153956  \n",
      "min        0.692308     32.540000   -124.350000      0.149990  \n",
      "25%        2.429741     33.930000   -121.800000      1.196000  \n",
      "50%        2.818116     34.260000   -118.490000      1.797000  \n",
      "75%        3.282261     37.710000   -118.010000      2.647250  \n",
      "max     1243.333333     41.950000   -114.310000      5.000010  \n"
     ]
    }
   ],
   "source": [
    "# Load California Housing dataset from scikit-learn\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# Fetch the dataset\n",
    "housing_data = fetch_california_housing(as_frame=True)\n",
    "df = housing_data.frame\n",
    "\n",
    "# Rename target for clarity\n",
    "df = df.rename(columns={\"MedHouseVal\": \"house_value\"})\n",
    "\n",
    "print(f\"Loaded {len(df)} housing records\")\n",
    "print(\"\\nFeature names:\", housing_data.feature_names)\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "print(\"\\nSummary statistics:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6e07a9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 1: Statistical Modeling with statsmodels\n",
    "\n",
    "**Note:** This question focuses on statistical modeling for inference - understanding relationships between variables. We'll use a subset of features (`MedInc`, `AveBedrms`, `Population`) to focus on interpretability and statistical significance rather than maximizing prediction accuracy. The `statsmodels` library provides detailed statistical information (p-values, confidence intervals, AIC) that helps us understand *why* variables are related.\n",
    "\n",
    "**Why a subset of features?** In statistical modeling, we often use fewer features to maintain interpretability and focus on understanding relationships. This contrasts with machine learning (Question 2), where we use all available features to maximize prediction accuracy.\n",
    "\n",
    "**Objective:** Fit linear regression models using `statsmodels`, extract statistical information, and compare models with and without interaction terms.\n",
    "\n",
    "### Part 1.1: Fit the Model\n",
    "\n",
    "Fit a linear regression model predicting `house_value` from `MedInc`, `AveBedrms`, and `Population` using the formula API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "03cc421e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a linear regression model using statsmodels formula API\n",
    "model = smf.ols('house_value ~ MedInc + AveBedrms + Population', data = df)  \n",
    "results = model.fit()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ad3971",
   "metadata": {},
   "source": [
    "### Part 1.2: Extract Model Summary\n",
    "\n",
    "Print the model summary and save key statistics to a text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cf0104e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Model Summary ===\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:            house_value   R-squared:                       0.474\n",
      "Model:                            OLS   Adj. R-squared:                  0.474\n",
      "Method:                 Least Squares   F-statistic:                     6205.\n",
      "Date:                Sat, 29 Nov 2025   Prob (F-statistic):               0.00\n",
      "Time:                        15:04:35   Log-Likelihood:                -25607.\n",
      "No. Observations:               20640   AIC:                         5.122e+04\n",
      "Df Residuals:                   20636   BIC:                         5.125e+04\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept      0.5084      0.021     24.089      0.000       0.467       0.550\n",
      "MedInc         0.4178      0.003    136.014      0.000       0.412       0.424\n",
      "AveBedrms     -0.0144      0.012     -1.165      0.244      -0.039       0.010\n",
      "Population  -2.89e-05   5.15e-06     -5.608      0.000    -3.9e-05   -1.88e-05\n",
      "==============================================================================\n",
      "Omnibus:                     4188.894   Durbin-Watson:                   0.659\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             9119.557\n",
      "Skew:                           1.177   Prob(JB):                         0.00\n",
      "Kurtosis:                       5.249   Cond. No.                     7.23e+03\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 7.23e+03. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n",
      "\n",
      "=== Coefficient Significance (p-values) ===\n",
      "\n",
      "=== All p-values ===\n",
      "\n",
      "Intercept     1.811739e-126\n",
      "MedInc         0.000000e+00\n",
      "AveBedrms      2.440565e-01\n",
      "Population     2.078771e-08\n",
      "dtype: float64\n",
      "\n",
      "=== Statistically significant (<0.05) p-values ===\n",
      "\n",
      "Statistically significant coefficient: Intercept || p-value: 0.00\n",
      "\n",
      "Statistically significant coefficient: MedInc || p-value: 0.00\n",
      "\n",
      "Statistically significant coefficient: Population || p-value: 0.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the model summary\n",
    "print(\"=== Model Summary ===\")\n",
    "print(results.summary())\n",
    "\n",
    "# Extract p-values for coefficients\n",
    "# Print which coefficients are statistically significant (p < 0.05)\n",
    "\n",
    "pvalues = results.pvalues \n",
    "print(\"\\n=== Coefficient Significance (p-values) ===\\n\")\n",
    "print(\"=== All p-values ===\\n\")\n",
    "print(pvalues)\n",
    "print(\"\\n=== Statistically significant (<0.05) p-values ===\\n\")\n",
    "for coefficient, pvalue in pvalues.items():\n",
    "    if pvalue < 0.05:\n",
    "        print(f\"Statistically significant coefficient: {coefficient} || p-value: {pvalue:.2f}\\n\")\n",
    "\n",
    "# Save key statistics to output file\n",
    "# Extract: R-squared, number of observations, and AIC (Akaike Information Criterion)\n",
    "# Format: \"R-squared: X.XXXX\\nObservations: XXXX\\nAIC: XXXXX.XX\"\n",
    "\n",
    "with open(\"output/q1_model_summary.txt\", \"w\") as f:\n",
    "    f.write(f\"R-squared: {results.rsquared:.4f}\\n\")\n",
    "    f.write(f\"Observations: {int(results.nobs)}\\n\")\n",
    "    f.write(f\"AIC: {results.aic:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355071a0",
   "metadata": {},
   "source": [
    "### Part 1.3: Make Predictions\n",
    "\n",
    "Make predictions for all houses and save to CSV.\n",
    "\n",
    "**Note:** In statistical modeling, we often make predictions on the full dataset to understand model fit. This differs from machine learning (Question 2), where we use train/test splits to evaluate generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8530a3f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved 20640 predictions to output/q1_statistical_model.csv\n"
     ]
    }
   ],
   "source": [
    "# Make predictions using the fitted model\n",
    "# The features are: MedInc, AveBedrms, Population\n",
    "# Save predictions along with actual values to CSV\n",
    "predictions = results.predict()  \n",
    "\n",
    "# Create DataFrame with predictions\n",
    "pred_df = pd.DataFrame({\n",
    "    \"actual_value\": df[\"house_value\"],\n",
    "    \"predicted_value\": predictions,\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "pred_df.to_csv(\"output/q1_statistical_model.csv\", index=False)\n",
    "print(f\"\\nSaved {len(pred_df)} predictions to output/q1_statistical_model.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e053bf61",
   "metadata": {},
   "source": [
    "### Part 1.4: Model with Interaction Term\n",
    "\n",
    "Now let's fit a model with an interaction term. An **interaction term** allows the effect of one variable to depend on the value of another variable. For example, the effect of income (`MedInc`) on house value might depend on the number of bedrooms (`AveBedrms`). In the formula API, we use `*` to include both main effects and their interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3c296d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model with Interaction Term ===\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:            house_value   R-squared:                       0.474\n",
      "Model:                            OLS   Adj. R-squared:                  0.474\n",
      "Method:                 Least Squares   F-statistic:                     4655.\n",
      "Date:                Sat, 29 Nov 2025   Prob (F-statistic):               0.00\n",
      "Time:                        15:04:36   Log-Likelihood:                -25605.\n",
      "No. Observations:               20640   AIC:                         5.122e+04\n",
      "Df Residuals:                   20635   BIC:                         5.126e+04\n",
      "Df Model:                           4                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "====================================================================================\n",
      "                       coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------------\n",
      "Intercept            0.4539      0.038     11.841      0.000       0.379       0.529\n",
      "MedInc               0.4338      0.010     43.852      0.000       0.414       0.453\n",
      "AveBedrms            0.0357      0.032      1.119      0.263      -0.027       0.098\n",
      "MedInc:AveBedrms    -0.0150      0.009     -1.703      0.089      -0.032       0.002\n",
      "Population       -2.871e-05   5.16e-06     -5.570      0.000   -3.88e-05   -1.86e-05\n",
      "==============================================================================\n",
      "Omnibus:                     4183.408   Durbin-Watson:                   0.658\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             9076.475\n",
      "Skew:                           1.178   Prob(JB):                         0.00\n",
      "Kurtosis:                       5.238   Cond. No.                     1.57e+04\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 1.57e+04. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "# Fit a model with an interaction term between MedInc and AveBedrms\n",
    "model_interaction = smf.ols('house_value ~ MedInc * AveBedrms + Population', data = df) \n",
    "results_interaction = model_interaction.fit()  \n",
    "print(\"\\n=== Model with Interaction Term ===\")\n",
    "print(results_interaction.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83bc2c1",
   "metadata": {},
   "source": [
    "### Part 1.5: Compare Models\n",
    "\n",
    "Compare the two models using AIC (Akaike Information Criterion). Lower AIC indicates a better model (accounting for model complexity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "10941b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model Comparison ===\n",
      "Simple model AIC: 51221.28\n",
      "Interaction model AIC: 51220.38\n",
      "The interaction model is better, as it has a lower AIC: 51220.38\n"
     ]
    }
   ],
   "source": [
    "# Compare the two models using AIC\n",
    "# Extract AIC from both models: results.aic and results_interaction.aic\n",
    "# Determine which model is better (lower AIC is better)\n",
    "\n",
    "aic_simple = results.aic  \n",
    "aic_interaction = results_interaction.aic  \n",
    "\n",
    "print(\"\\n=== Model Comparison ===\")\n",
    "print(f\"Simple model AIC: {aic_simple:.2f}\")\n",
    "print(f\"Interaction model AIC: {aic_interaction:.2f}\")\n",
    "if aic_simple > aic_interaction:\n",
    "    print(f\"The interaction model is better, as it has a lower AIC: {aic_interaction:.2f}\")\n",
    "elif aic_simple < aic_interaction:\n",
    "    print(f\"The simple model is better, as it has a lower AIC: {aic_simple:.2f}\")\n",
    "else:\n",
    "    print(f\"Both models perform the same, as they both have an AIC of: {aic_simple:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e744c660",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 2: Machine Learning with scikit-learn\n",
    "\n",
    "**Note:** While Question 1 focused on statistical inference (understanding relationships and testing hypotheses), Question 2 focuses on machine learning for prediction. We'll use all available features to maximize prediction accuracy rather than focusing on interpretability.\n",
    "\n",
    "**Objective:** Fit and compare linear regression and random forest models using `scikit-learn`.\n",
    "\n",
    "### Part 2.1: Prepare Data\n",
    "\n",
    "Split the data into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8f8c6591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 16512 samples\n",
      "Test set: 4128 samples\n"
     ]
    }
   ],
   "source": [
    "# Prepare features and target\n",
    "# Features: ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude']\n",
    "# Target: 'house_value'\n",
    "\n",
    "feature_cols = [\n",
    "    \"MedInc\",\n",
    "    \"HouseAge\",\n",
    "    \"AveRooms\",\n",
    "    \"AveBedrms\",\n",
    "    \"Population\",\n",
    "    \"AveOccup\",\n",
    "    \"Latitude\",\n",
    "    \"Longitude\",\n",
    "]\n",
    "\n",
    "X = df[feature_cols]  \n",
    "y = df[\"house_value\"] \n",
    "\n",
    "# Split into train and test sets (80/20 split, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42) \n",
    "\n",
    "print(f\"Training set: {len(X_train)} samples\")\n",
    "print(f\"Test set: {len(X_test)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7c89cb",
   "metadata": {},
   "source": [
    "### Part 2.2: Fit Linear Regression\n",
    "\n",
    "Fit a linear regression model and evaluate it on both training and test sets. Comparing train and test performance helps us detect overfitting - if the model performs much better on training data than test data, it's likely overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d7e5aa7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Linear Regression Results ===\n",
      "Training - R²: 0.6126, RMSE: 0.72\n",
      "Test - R²: 0.5758, RMSE: 0.75\n"
     ]
    }
   ],
   "source": [
    "# Fit a LinearRegression model\n",
    "lr_model = LinearRegression()  \n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Make predictions on both training and test sets\n",
    "lr_train_pred = lr_model.predict(X_train) \n",
    "lr_test_pred = lr_model.predict(X_test)  \n",
    "\n",
    "# Calculate metrics on both sets\n",
    "lr_train_r2 = r2_score(y_train, lr_train_pred)\n",
    "lr_test_r2 = r2_score(y_test, lr_test_pred)\n",
    "lr_train_rmse = np.sqrt(mean_squared_error(y_train, lr_train_pred))\n",
    "lr_test_rmse = np.sqrt(mean_squared_error(y_test, lr_test_pred))\n",
    "\n",
    "print(\"=== Linear Regression Results ===\")\n",
    "print(f\"Training - R²: {lr_train_r2:.4f}, RMSE: {lr_train_rmse:.2f}\")\n",
    "print(f\"Test - R²: {lr_test_r2:.4f}, RMSE: {lr_test_rmse:.2f}\")\n",
    "\n",
    "# Store test predictions for later use\n",
    "lr_pred = lr_test_pred\n",
    "lr_r2 = lr_test_r2\n",
    "lr_rmse = lr_test_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7036c8bd",
   "metadata": {},
   "source": [
    "### Part 2.3: Fit Random Forest\n",
    "\n",
    "Fit a random forest model and evaluate it on both training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2b8f4dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Random Forest Results ===\n",
      "Training - R²: 0.8050, RMSE: 0.51\n",
      "Test - R²: 0.7389, RMSE: 0.58\n"
     ]
    }
   ],
   "source": [
    "# Fit a RandomForestRegressor model\n",
    "# Use: n_estimators=50, max_depth=8, random_state=42\n",
    "rf_model = RandomForestRegressor(n_estimators=50, max_depth=8, random_state=42)  \n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on both training and test sets\n",
    "rf_train_pred = rf_model.predict(X_train) \n",
    "rf_test_pred = rf_model.predict(X_test) \n",
    "\n",
    "# Calculate metrics on both sets\n",
    "rf_train_r2 = r2_score(y_train, rf_train_pred)\n",
    "rf_test_r2 = r2_score(y_test, rf_test_pred)\n",
    "rf_train_rmse = np.sqrt(mean_squared_error(y_train, rf_train_pred))\n",
    "rf_test_rmse = np.sqrt(mean_squared_error(y_test, rf_test_pred))\n",
    "\n",
    "print(\"=== Random Forest Results ===\")\n",
    "print(f\"Training - R²: {rf_train_r2:.4f}, RMSE: {rf_train_rmse:.2f}\")\n",
    "print(f\"Test - R²: {rf_test_r2:.4f}, RMSE: {rf_test_rmse:.2f}\")\n",
    "\n",
    "# Store test predictions and metrics for later use\n",
    "rf_pred = rf_test_pred\n",
    "rf_r2 = rf_test_r2\n",
    "rf_rmse = rf_test_rmse\n",
    "\n",
    "# Extract feature importance for later comparison\n",
    "rf_feature_importance = rf_model.feature_importances_ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb07dec4",
   "metadata": {},
   "source": [
    "### Part 2.4: Save Predictions and Comparison\n",
    "\n",
    "Save predictions and model comparison to files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "610aef44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved predictions to output/q2_ml_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "# Save predictions to CSV\n",
    "# Include: actual_value, lr_predicted_value, rf_predicted_value\n",
    "pred_df = pd.DataFrame({\n",
    "    \"actual_value\": y_test.values,\n",
    "    \"lr_predicted_value\": lr_pred,\n",
    "    \"rf_predicted_value\": rf_pred,\n",
    "})\n",
    "\n",
    "pred_df.to_csv(\"output/q2_ml_predictions.csv\", index=False)\n",
    "print(f\"\\nSaved predictions to output/q2_ml_predictions.csv\")\n",
    "\n",
    "# Save model comparison to text file\n",
    "# Include both train and test metrics\n",
    "# Format: \"Linear Regression - Train R²: X.XXXX, Test R²: X.XXXX, Test RMSE: XX.XX\\nRandom Forest - Train R²: X.XXXX, Test R²: X.XXXX, Test RMSE: XX.XX\"\n",
    "with open(\"output/q2_model_comparison.txt\", \"w\") as f:\n",
    "    f.write(f\"Linear Regression - Train R²: {lr_train_r2:.4f}, Test R²: {lr_r2:.4f}, Test RMSE: {lr_rmse:.2f}\\n\")\n",
    "    f.write(f\"Random Forest - Train R²: {rf_train_r2:.4f}, Test R²: {rf_r2:.4f}, Test RMSE: {rf_rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f7ae27",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 3: Gradient Boosting with XGBoost\n",
    "\n",
    "**Note:** Question 3 introduces gradient boosting, an advanced machine learning technique that often achieves the best performance on tabular data. XGBoost builds models sequentially, with each new model learning from the mistakes of previous ones.\n",
    "\n",
    "**Objective:** Fit an XGBoost model and extract feature importance.\n",
    "\n",
    "### Part 3.1: Fit XGBoost Model\n",
    "\n",
    "Fit an XGBoost regressor model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c3c839d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost - R²: 0.7918, RMSE: 0.52\n"
     ]
    }
   ],
   "source": [
    "# Fit an XGBRegressor model\n",
    "# Use: n_estimators=100, max_depth=3, learning_rate=0.15, random_state=42\n",
    "xgb_model = xgb.XGBRegressor(n_estimators=100, max_depth=3, learning_rate=0.15, random_state=42)  # Replace None with your code\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test set\n",
    "xgb_pred = xgb_model.predict(X_test)  # Replace None with your code\n",
    "\n",
    "# Calculate metrics\n",
    "xgb_r2 = r2_score(y_test, xgb_pred)\n",
    "xgb_rmse = np.sqrt(mean_squared_error(y_test, xgb_pred))\n",
    "\n",
    "print(f\"XGBoost - R²: {xgb_r2:.4f}, RMSE: {xgb_rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22c8744",
   "metadata": {},
   "source": [
    "### Part 3.2: Extract and Compare Feature Importance\n",
    "\n",
    "Extract feature importance from XGBoost and compare it with Random Forest from Question 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "458210b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== XGBoost Feature Importance ===\n",
      "      feature  xgb_importance\n",
      "0      MedInc        0.567666\n",
      "5    AveOccup        0.154247\n",
      "7   Longitude        0.074295\n",
      "1    HouseAge        0.067540\n",
      "6    Latitude        0.057086\n",
      "2    AveRooms        0.053436\n",
      "3   AveBedrms        0.014782\n",
      "4  Population        0.010947\n",
      "\n",
      "=== Feature Importance Comparison ===\n",
      "      feature  random_forest   xgboost\n",
      "0      MedInc       0.644830  0.567666\n",
      "5    AveOccup       0.140574  0.154247\n",
      "7   Longitude       0.060245  0.074295\n",
      "1    HouseAge       0.046593  0.067540\n",
      "6    Latitude       0.062580  0.057086\n",
      "2    AveRooms       0.025040  0.053436\n",
      "3   AveBedrms       0.009721  0.014782\n",
      "4  Population       0.010418  0.010947\n"
     ]
    }
   ],
   "source": [
    "# Extract feature importance from XGBoost\n",
    "# Use: xgb_model.feature_importances_\n",
    "xgb_feature_importance = xgb_model.feature_importances_  \n",
    "\n",
    "# Create DataFrame for XGBoost importance\n",
    "xgb_importance_df = pd.DataFrame({\n",
    "    \"feature\": feature_cols,\n",
    "    \"xgb_importance\": xgb_feature_importance,\n",
    "}).sort_values(\"xgb_importance\", ascending=False)\n",
    "\n",
    "print(\"\\n=== XGBoost Feature Importance ===\")\n",
    "print(xgb_importance_df)\n",
    "\n",
    "# Compare with Random Forest feature importance\n",
    "# Create a comparison DataFrame with both models' feature importance\n",
    "# Sort by XGBoost importance for display\n",
    "importance_comparison = pd.DataFrame({\n",
    "    \"feature\": feature_cols,\n",
    "    \"random_forest\": rf_feature_importance,\n",
    "    \"xgboost\": xgb_feature_importance,\n",
    "}).sort_values(\"xgboost\", ascending=False)\n",
    "\n",
    "print(\"\\n=== Feature Importance Comparison ===\")\n",
    "print(importance_comparison)\n",
    "\n",
    "# Save XGBoost feature importance to text file\n",
    "# Format: \"feature_name: X.XXXX\" (one per line, sorted by importance)\n",
    "with open(\"output/q3_feature_importance.txt\", \"w\") as f:\n",
    "    for feature, importance in zip(xgb_importance_df['feature'], xgb_importance_df['xgb_importance']):\n",
    "        f.write(f\"{feature}: {importance:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3349430",
   "metadata": {},
   "source": [
    "### Part 3.3: Save Predictions\n",
    "\n",
    "Save XGBoost predictions to CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8b8a4dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved predictions to output/q3_xgboost_model.csv\n"
     ]
    }
   ],
   "source": [
    "# Save predictions to CSV\n",
    "# Include: actual_value, xgb_predicted_value\n",
    "pred_df = pd.DataFrame({\n",
    "    \"actual_value\": y_test.values,\n",
    "    \"xgb_predicted_value\": xgb_pred,\n",
    "})\n",
    "\n",
    "pred_df.to_csv(\"output/q3_xgboost_model.csv\", index=False)\n",
    "print(f\"\\nSaved predictions to output/q3_xgboost_model.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e9bcb1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Submission Checklist\n",
    "\n",
    "Before submitting, verify you've created all required output files:\n",
    "\n",
    "- [ ] `output/q1_statistical_model.csv`\n",
    "- [ ] `output/q1_model_summary.txt`\n",
    "- [ ] `output/q2_ml_predictions.csv`\n",
    "- [ ] `output/q2_model_comparison.txt`\n",
    "- [ ] `output/q3_xgboost_model.csv`\n",
    "- [ ] `output/q3_feature_importance.txt`\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
